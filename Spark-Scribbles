Reference video:-
https://www.youtube.com/watch?v=7ooZ4S7Ay6Y&t=1524s

Slides:-
https://www.youtube.com/redirect?v=7ooZ4S7Ay6Y&redir_token=GLhppjsOmi1_kM-PV7q2FTeFjzt8MTUzNzk3NzQzMUAxNTM3ODkxMDMx&event=video_description&q=https%3A%2F%2Fspark-summit.org%2Fwp-content%2Fuploads%2F2015%2F03%2FSparkSummitEast2015-AdvDevOps-StudentSlides.pdf

Caching an RDD :-
Caching an RDD will just a lazy transformation determine which rdd should be cached based on how many times we are reuisng the RDD.
The percentage of the performance increase depends on how much we are able to cache

Q : Where will cache RDD be stored ?
Ans:-

Q: How will spark work on per partition basis ?
Ans:-


Q:-What is a joined RDD and CassandraRDD?
Ans:-

White Paper on Sparks:-

https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf

berkley white paper on RDD's
https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf

white paper on spark streaming
http://sigops.org/s/conferences/sosp/2013/papers/p423-zaharia.pdf

splitSize vs blockSize in hadoop
https://stackoverflow.com/questions/30549261/split-size-vs-block-size-in-hadoop

Recommended memory Usages:-
Use at most 75% of a machines memory for Spark
Minimum executor heap size should be 8Gb
Max executor heap size depends... may be 40GB(watchout for GC)
Memory Usage is greatly affected by storage level and serialization format
